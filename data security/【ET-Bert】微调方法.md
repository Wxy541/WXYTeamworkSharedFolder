### 📄 3.4 微调 ET-BERT

微调能够很好地服务于下游分类任务，原因如下：

1. **预训练所学习到的表征与流量类别无关**，因此可适用于任意类型的网络流量表示；
2. 由于预训练模型的输入是**数据报字节级别**（datagram bytes level）的，因此那些需要对**数据包**（packets）的下游任务，都可以将其转换为对应的**数据报字节 token 序列**，交由该模型进行分类；
3. 预训练模型输出中的特殊 **[CLS] token** 能够建模整个输入流量的全局表征，可直接用于分类任务。

由于微调阶段与预训练阶段的模型结构基本一致，我们将面向具体任务的数据包或流表示输入到预训练好的 ET-BERT 模型中，并以端到端的方式微调所有参数。在输出层，将 [CLS] token 对应的向量送入一个多类分类器进行预测。

我们提出了两种微调策略，以适应不同场景下的分类需求：

1. **以数据包级别**（packet level）：用于验证 ET-BERT 是否能适应更细粒度的流量数据，记为 **ET-BERT(packet)**；
2. **以流级别**（flow level）：用于与现有方法进行公平、客观的比较，记为 **ET-BERT(flow)**。

这两种微调模型的主要区别在于**输入流量所包含的信息量不同**。  
我们采用**一个流中连续 M 个数据包拼接而成的数据报**（stitched datagram）作为输入，其中 M 在本文中设为 5。具体的流量数据处理细节见第 4.1 节。

相比预训练，微调的计算开销相对较低，单块 GPU 即可完成微调任务。

---

### 🔍 举例说明（便于理解）

#### 场景设定：

假设我们要用 ET-BERT 识别网络流量类型，比如区分：
- 正常网页浏览（HTTP）
- 视频流（YouTube）
- 恶意软件通信（C2 流量）

---

#### ✅ 例子 1：**ET-BERT(packet) —— 包级别微调**

- **输入**：单个原始网络数据包的前 64 字节（如 TCP payload 的开头部分），转换为字节 token 序列。
- **用途**：判断这个**单独的数据包**属于哪一类应用。
- **适用场景**：实时入侵检测系统（IDS），需要对每个到达的数据包快速分类。
- **挑战**：单个包信息有限（比如一个 TCP ACK 包几乎无 payload），分类难度大。
- **论文目的**：验证 ET-BERT 是否具备从极细粒度数据中提取语义的能力。

> 💡 类比：就像只给你一句话的前 10 个字，让你猜整篇文章的主题。

---

#### ✅ 例子 2：**ET-BERT(flow) —— 流级别微调（主实验设置）**

- **输入**：从同一个网络流（如某 IP 对之间的通信）中取出**连续 5 个数据包**，把它们的字节内容**拼接起来**（stitch），形成一个更长的字节序列（例如 5 × 64 = 320 字节）。
- **用途**：判断这个**完整通信流**属于哪种应用类型。
- **优势**：包含更多上下文（如请求 + 响应 + 数据传输），分类更准确。
- **公平比较**：大多数已有方法（如 DeepPacket、FlowPic）也是基于“流”做分类，因此用 flow 级别输入才能公平对比。

> 💡 类比：给你一段连续对话（5 句话），让你判断这是客服聊天、视频通话还是黑客指令。

---

#### 🧠 关于 [CLS] token 的作用

- ET-BERT 在输入序列开头加了一个特殊标记 `[CLS]`。
- 经过 Transformer 编码后，`[CLS]` 位置的输出向量会**聚合整个输入序列的语义信息**。
- 微调时，就拿这个向量去做分类（接一个全连接层 + softmax）。

> 这和 BERT 在 NLP 中的做法完全一致：用 [CLS] 表示整句话的 embedding。

---

### ✅ 总结关键点

|特性|说明|
|---|---|
|**输入形式**|原始字节（0~255），无需手工特征|
|**预训练通用性**|学到的是通用流量模式，不依赖特定类别|
|**微调灵活**|支持包级（细粒度）和流级（常规）两种输入|
|**[CLS] 用途**|代表整个输入的语义，直接用于分类|
|**计算成本低**|微调只需单 GPU，适合实际部署|

---

这种设计使得 ET-BERT 既能作为通用流量表征模型，又能灵活适配不同粒度的网络安全任务，是其在流量分析领域的重要创新之一。