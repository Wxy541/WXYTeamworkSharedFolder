捕捉token的上下文关系

### 🧩 1. 假设原始输入序列（来自 Section 3.2 的 token 化结果）

假设经过 **Datagram2Token** 处理后，我们得到一个长度为 5 的 token 序列（已加入特殊 token）：

[ X = [\text{[CLS]},\ 5635,\ 258,\ 20,\ \text{[SEP]}] ]

- 所有 token 都是整数 ID（例如 `5635` 是 bi-gram `0x1603` 的十进制表示）
- 序列长度 ( n = 5 )

---

### 🎭 2. 应用 MBM 掩码策略（15% 概率）

对每个 token 独立地以 **15% 概率**决定是否掩码。

假设我们随机选中 **第 2 个和第 3 个普通 token**（即位置 1 和 2）进行掩码：

- 位置 1: token = `5635`
- 位置 2: token = `258`

> 其他位置（[CLS], 20, [SEP]）未被选中，保持不变。

对每个被选中的 token，按以下规则替换：

- **80%**: 替换为 `[MASK]`
- **10%**: 替换为**随机其他 token**
- **10%**: **保持不变**

假设：

- 位置 1（`5635`）→ 被替换为 `[MASK]`（80% 情况）
- 位置 2（`258`）→ 被替换为随机 token，比如 `9999`（10% 情况）

于是，**掩码后的输入序列** (\overline{X}) 变为：

[ \overline{X} = [\text{[CLS]},\ \text{[MASK]},\ 9999,\ 20,\ \text{[SEP]}] ]

> 注意：只有被掩码的 token（位置 1 和 2）参与损失计算，即使位置 2 被替换成随机值或保留原值，模型仍需预测其**原始真实值**（即 `5635` 和 `258`）。

---

### 🔍 3. 模型前向传播

将 (\overline{X}) 输入 ET-BERT（经过 Token2Embedding + Transformer 编码器）。

Transformer 输出每个位置的上下文表示。我们只关心**被掩码位置**的输出 logits。

- 在位置 1，模型输出一个词汇表上的概率分布 (P_1)，用于预测原始 token。
- 在位置 2，输出分布 (P_2)。

假设词汇表大小 (V = 65536 + \text{special tokens} \approx 65600)。

模型使用一个 **输出分类层**（通常是一个与 token embedding 权重绑定的线性层）将 768 维隐藏状态映射到 (V) 维 logits，再经 softmax 得到概率。

---

### 📉 4. 计算损失（Negative Log-Likelihood）

损失函数为：

[ L_{\text{MBM}} = - \sum_{i=1}^{k} \log P(\text{MASK}_i = \text{token}_i \mid \overline{X}; \theta) ]

其中 (k = 2)（两个被掩码的位置）。

#### 假设模型预测概率如下：

- 在位置 1（应预测 `5635`）：
    - 模型给出 (P_1(5635) = 0.7)
- 在位置 2（应预测 `258`）：
    - 模型给出 (P_2(258) = 0.4)

则损失为：

[ \begin{align*} L_{\text{MBM}} &= - \left[ \log(0.7) + \log(0.4) \right] \ &= - \left[ -0.3567 + (-0.9163) \right] \ &= - (-1.273) \ &= 1.273 \end{align*} ]

> （使用自然对数 ln；若用 log₁₀ 则数值不同，但论文通常用自然对数或默认 torch.nn.CrossEntropyLoss 的 log_softmax + nll）

在实际实现中，这等价于对两个位置分别计算 **交叉熵损失**，然后求和。

---

### ✅ 5. 关键点总结

|项目|说明|
|---|---|
|**掩码对象**|原始 token 序列中的普通流量 token（非特殊 token）|
|**掩码比例**|每个 token 独立以 15% 概率被选中|
|**替换策略**|80% → `[MASK]`，10% → 随机 token，10% → 原 token|
|**训练目标**|无论怎么替换，都让模型**预测原始真实 token**|
|**损失计算**|仅对被选中的掩码位置计算负对数似然|
|**优势**|强迫模型学习**双向上下文依赖**，捕捉加密流量中字节间的隐式模式|

---

### 💡 补充说明

- 即使某个被掩码的 token 被替换成**随机值**（如 `9999`），模型依然要预测它**原本是 `258`**。这增加了任务难度，防止模型“偷看”输入。
- `[CLS]` 和 `[SEP]` 通常**不参与掩码**（实践中常排除特殊 token）。
- 这种设计借鉴 BERT，但应用于**无语义的加密流量字节序列**，是 ET-BERT 的核心创新之一。

---

希望这个例子能清晰展示 MBM 的训练机制和损失计算过程！