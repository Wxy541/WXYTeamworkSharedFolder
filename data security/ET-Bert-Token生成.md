### **实例说明：从原始流量到 Token**

#### **步骤 1：从原始流量中提取 BURST**

假设我们有一个客户端（Client）和服务器（Server）之间的 TLS 加密会话。在这个会话中，客户端连续发送了 3 个数据包（没有收到服务器回复），紧接着服务器连续回复了 2 个数据包。

- **客户端到服务器的数据包（Client-to-Server）**：
    
    - Packet 1: `16 03 01 02 00 ...`（十六进制表示的原始字节）
    - Packet 2: `14 03 03 00 16 ...`
    - Packet 3: `17 03 03 01 20 ...`
- **服务器到客户端的数据包（Server-to-Client）**：
    
    - Packet 4: `16 03 03 00 45 ...`
    - Packet 5: `17 03 03 03 00 ...`

根据论文 **3.2.1 BURST Generator** 的定义，一个 BURST 是**同一方向上连续发送的数据包集合**。因此，我们可以得到两个 BURST：

- **BURST A (Client-to-Server)**:
    
    - 合并 Packet 1, 2, 3 的 payload（去掉 IP/TCP 头后）：
        
        ```
        16 03 01 02 00 ... 14 03 03 00 16 ... 17 03 03 01 20 ...
        ```
        
- **BURST B (Server-to-Client)**:
    
    - 合并 Packet 4, 5 的 payload：
        
        ```
        16 03 03 00 45 ... 17 03 03 03 00 ...
        ```
        

> 💡 注意：实际中 BURST 可能只取前 N 个字节（例如 512 字节），以控制输入长度。

---

#### **步骤 2：将 BURST 转换为 Token（BURST2Token）**

根据 **3.2.2 BURST2Token**，使用 **bi-gram（两个字节一组）** 来编码十六进制序列。

我们以 **BURST A 的开头部分**为例：

原始字节序列（十六进制）：

```
16 03 01 02 00 14 03 03 00 16 ...
```

按 **bi-gram（每两个字节）** 分组：

- `16 03` → 十六进制 `0x1603` → 十进制 **5635**
- `01 02` → `0x0102` → **258**
- `00 14` → `0x0014` → **20**
- `03 03` → `0x0303` → **771**
- `00 16` → `0x0016` → **22**
- ...

> 📌 论文提到：“each unit consists of two adjacent bytes... each token unit ranges from 0 to 65535”，即每个 token 是一个 16 位无符号整数（0~65535），对应一个 bi-gram。

因此，BURST A 被转换为一个 **token 序列**：

```
[5635, 258, 20, 771, 22, ...]
```

同样，BURST B 也会被转换为类似的 token 序列。

---

#### **步骤 3：构造模型输入（加入特殊 token 和分段）**

为了进行预训练（特别是 SBP 任务），ET-BERT 会将一个完整的 BURST **等分为两个子 BURST**（sub-BURST A 和 sub-BURST B），并在它们之间插入特殊 token `[SEP]`。

假设我们将 BURST A 的 token 序列截断并分为两半：

- **sub-BURST A**: `[5635, 258, 20]`
- **sub-BURST B**: `[771, 22, ...]`

然后构造完整的输入序列（用于预训练）：

```
[CLS] 5635 258 20 [SEP] 771 22 ... [PAD] [PAD] ...
```

其中：

- `[CLS]`：序列开始，用于分类任务（其最终隐藏状态代表整个序列）。
- `[SEP]`：分隔两个子 BURST。
- `[PAD]`：填充至固定长度（如 512 tokens）。

> 🔒 注意：在 **Masked BURST Model (MBM)** 预训练任务中，部分 token（如 `258`）会被随机替换为 `[MASK]`，模型需要预测原始值。

---

### **总结：Token 生成流程**

1. **原始流量** → 按方向分割为 **BURST**（连续同向数据包合并）。
2. **BURST 的原始字节**（十六进制）→ 按 **bi-gram（每两个字节）** 分组。
3. 每个 bi-gram → 转换为 **0~65535 的整数 token**。
4. 加入 `[CLS]`、`[SEP]`、`[PAD]` 等特殊 token，形成最终输入序列。

这种方式将**无语义的加密流量字节**转换为类似“语言”的 token 序列，使得 Transformer 模型能够通过上下文学习其隐含模式。

