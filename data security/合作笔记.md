任务可能不一定可“并行”合作，咱们至少可以“并发”合作，大家先考察一下自己最近是否有空，然后在群里声明一下要接哪个任务，且计划什么时候完成

示例格式：`xxx(姓名) 进行任务“【优先】ET-Bert环境要求调研” 截止11-06晚上`

两人都想要接同一个任务，谁认为自己能够优先完成任务谁就接，且在完成任务后，补充好本文档的记录，任务进行到了下一步或者遇到问题可在“当前任务列表”中补充，方便协作；并且**要在群里及时通知一声！**


当前任务列表：（记录当前未完成的任务和未解决的问题，实时更新）
1. 【进行中-z】环境复现
2. 【未完成】复现4.2实验仅关于ET-Bert的部分（ET-BERT(flow)和ET-BERT(packet)）
3. 【已完成-z】【优先】ET-Bert环境要求调研（实在调研不清楚的项目，就标记为【未知】，但是不要留空）
4. 【已完成-h】整体实验调研（简单了解到底做了几个实验，补充下述“实验设计”部分的任务规划，不必了解具体细节）

## 进度规划
第一版：复现4.2实验仅关于ET-Bert的部分
第二版：复现4.3、4.4、4.5实验
第三版：复现4.2实验其它模型对比（需要搭建其它模型的环境）
第四版：尝试改进（非明确要求，需要对论文有深入理解，如果未来不从事流量分析方向，则不必强求）

## 环境搭建
调研环境需求，据此选择复现哪一篇

## 环境搭建
调研环境需求，据此选择复现哪一篇
### ET-Bert 环境要求
后续根据实际使用资源填写，做pre需要
#### 硬件环境
GPU显存：
【待补充】

数据集大小（一个一个调查，如果数据集较大，可以选择复现部分任务）：
【待补充】

数据处理后可能需要预留的容量：
【待补充】

#### 软件环境
主要是适配CUDA版本的相关Python包：
【待补充】

### 环境搭建笔记
此部分记录环境搭建时遇到的难题

## 实验设计与复现
简要描述论文中分别做了哪些实验？

### 复现方式简述
#### 1. 微调使用
##### 1.1 模型获取
您现在可以直接通过预训练[模型](https://drive.google.com/file/d/1r1yE34dU2W8zSqx1FkB8gCWri4DQWVtE/view?usp=sharing)使用 ET-BERT，也可以通过以下方式下载：

```
wget -O pretrained_model.bin https://drive.google.com/file/d/1r1yE34dU2W8zSqx1FkB8gCWri4DQWVtE/view?usp=sharing
```

##### 1.2 微调
获得预训练模型后，可以通过在数据包级别对标记的网络流量进行微调，将ET-BERT应用于任务：
```
python3 fine-tuning/run_classifier.py --pretrained_model_path models/pre-trained_model.bin \
                                   --vocab_path models/encryptd_vocab.txt \
                                   --train_path datasets/cstnet-tls1.3/packet/train_dataset.tsv \
                                   --dev_path datasets/cstnet-tls1.3/packet/valid_dataset.tsv \
                                   --test_path datasets/cstnet-tls1.3/packet/test_dataset.tsv \
                                   --epochs_num 10 --batch_size 32 --embedding word_pos_seg \
                                   --encoder transformer --mask fully_visible \
                                   --seq_length 128 --learning_rate 2e-5
```

##### 1.3 推理
然后，您可以使用微调后的模型进行推理：`models/finetuned_model.bin`
```
python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/encryptd_vocab.txt \
                                          --test_path datasets/cstnet-tls1.3/packet/nolabel_test_dataset.tsv \
                                          --prediction_path datasets/cstnet-tls1.3/packet/prediction.tsv \
                                          --labels_num 120 \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```


#### 2. 重新预训练
##### 2.1 数据预处理
要重现在网络流量数据上预训练 ET-BERT 所需的步骤，请执行以下步骤：

1. 运行生成加密的流量语料库或直接使用生成的语料库。请注意，您需要更改文件路径，并在文件顶部进行一些配置。`vocab_process/main.py``corpora/`
2. 运行以预处理加密的流量突发语料库。`main/preprocess.py`
    
    ```
       python3 preprocess.py --corpus_path corpora/encrypted_traffic_burst.txt \
                             --vocab_path models/encryptd_vocab.txt \
                             --dataset_path dataset.pt --processes_num 8 --target bert
    ```
    
3. 如果有需要处理的 pcap 格式数据集，则运行以生成下游任务的数据。此过程包括两个步骤。第一种是通过设置并另存为数据集来拆分 pcap 文件。然后是生成微调数据。如果使用共享数据集，则需要在命名下创建一个文件夹，并在此处复制数据集。`data_process/main.py``splitcap=True``datasets/main.py:54``npy``dataset_save_path``dataset`


##### 2.2 开始预训练
要重现在标记数据上微调 ET-BERT 所需的步骤，请运行以进行预训练。如果希望继续在已经预训练的模型上进行训练，可以增加参数 a。`pretrain.py``--pretrained_model_path`

```
   python3 pre-training/pretrain.py --dataset_path dataset.pt --vocab_path models/encryptd_vocab.txt \
                       --output_model_path models/pre-trained_model.bin \
                       --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                       --total_steps 500000 --save_checkpoint_steps 10000 --batch_size 32 \
                       --embedding word_pos_seg --encoder transformer --mask fully_visible --target bert
```



### 实验一（原4.2本模型的部分）
#### 实验设计
实验目的：测试模型的检测效果
涉及的数据集：Cross-Platform(iOS/Android)、ISCX-VPN-Service、ISCX-VPN-App、ISCX-Tor、USTC-TFC、CSTNET-TLS 1.3（分别使用flow与packet）

超参数：
- Epochs = 10
- 学习率：Flow-level = 6×10⁻⁵；Packet-level = 2×10⁻⁵
- Dropout = 0.5

使用到的指标：AC、PR、RC、F1

涉及图表：
每个数据集都关联一个2×4的表格，如：Cross-Platform(iOS)：

|                 | AC  | PR  | RC  | F1  |
| --------------- | --- | --- | --- | --- |
| ET-Bert(flow)   |     |     |     |     |
| ET-Bert(packet) |     |     |     |     |


#### 实验复现笔记
记录复现过程中遇到的难题，或者和论文结果不一致的情形

### 实验二（原4.4、4.5）


### 实验三（原4.3）


### 实验四（原4.2其它模型对比）


......
## 结合代码理解论文
以复现优先，不急着理解论文讲的是什么

可以参考如下笔记：
1. [[ET-Bert-Token生成]]
2. [[【ET-Bert】MBM预训练方法]]
3. [[【ET-Bert】SBP预训练]]
4. [[【ET-Bert】微调方法]]
5. [[【ET-Bert】数据集与实验设置]]

## 实验结论
根据具体的实验设计书写结论，每完成一个实验，需要及时填写

